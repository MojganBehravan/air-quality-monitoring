# Air Quality Monitoring - Batch Data Pipeline

This project implements a scalable, Docker-based batch data processing pipeline to monitor and analyze air pollution trends using the U.S. Pollution Dataset.

---

## üîß Architecture Overview

This modular pipeline follows a layered architecture:

1. **Ingestion Layer** (Python): Loads CSV data from Kaggle into HDFS.
2. **Storage Layer** (HDFS): Stores raw and processed data in a fault-tolerant file system.
3. **Processing Layer** (Spark + PySpark): Cleans, aggregates, and summarizes data.
4. **Data Layer** (MongoDB): Holds processed outputs for querying.
5. **Delivery Layer** (Flask API): Exposes endpoints for external use.
6. **Presentation Layer** (Streamlit): Interactive dashboard for data visualization.
7. **Orchestration Layer** (Apache Airflow): Manages automated scheduling.


---

## üóÇ Dataset

- Source: [U.S. Pollution Dataset ‚Äì Kaggle](https://www.kaggle.com/datasets/sogun3/uspollution)
- Format: CSV
- Size: 1.5M+ time-stamped records

---

## üîß Requirements

- Docker & Docker Compose
- Python 3.9+
- Spark
- Hadoop (via Docker images)
- MongoDB
- Apache Airflow
- Streamlit
- Flask

---

## üìä Endpoints (Flask)

| Endpoint       | Description                         |
|----------------|-------------------------------------|
| `http://localhost:5000/summary`     | Shows top summary of records        |
| `http://localhost:5000/filter?state=California&year=2005`      | Filter by state or year parameters  |

---

## üìà Visualization

The dashboard presents interactive charts:
- Pollution trends per state/year
- Average levels of NO2, O3, SO2, and CO

The dashboard is available at `http://localhost:8501`  
![Example Dashboard](https://github.com/MojganBehravan/air-quality-monitoring/raw/main/dashboard/dashboard.png)

---

## üöÄ How to Run the Project

Since the Storage Layer (HDFS) must be initialized before any data is ingested or processed, follow this sequence to ensure all required folders (`/raw`, `/processed`) exist before the pipeline starts.

### üì• Dataset Setup

Before running the pipeline, you must download the air pollution dataset:

1. Download it from Kaggle:  
   üëâ https://www.kaggle.com/datasets/sogun3/uspollution

2. Place the downloaded CSV file (e.g., `pollution_us_2000_2016.csv`) into the `data/` directory at the root of the project:
> ‚ö†Ô∏è Ensure the filename matches what the `ingest.py` script expects. If different, update the script accordingly.

### üß≠ Execution Steps

After cloning the repository:

```bash
# Step 0: Build the Docker images (only needed the first time or if Dockerfiles change)
docker-compose build

# Step 1: Start only HDFS components
docker-compose up -d namenode datanode

# Step 2: Run the HDFS setup script to create required folders
python ingestion/setup_hdfs.py

# Step 3: Ingest raw data to HDFS (this copies CSV to /user/hadoop/raw)
docker-compose up --no-deps --build ingestion

# Step 4: Run Spark to process the data and create summary.json
docker-compose up --no-deps --build spark
Check the summary file created:  docker exec -it namenode hdfs dfs -ls /user/hadoop/processed

# Step 5: Load summary.json into MongoDB
docker-compose up -d mongodb
docker-compose up --no-deps --build data_layer

# Step 7: Start the rest (API, dashboard, airflow)
docker-compose up -d airflow airflow-scheduler delivery dashboard

```

This ensures:
- `/user/hadoop/raw` and `/user/hadoop/processed` exist
- Spark and ingestion won‚Äôt fail due to missing HDFS paths
- A smooth and consistent startup process


## üë§ Author

Mojgan Behravan  


