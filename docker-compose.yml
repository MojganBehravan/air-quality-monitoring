services:
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=test
    ports:
      - "50570:50070"
      - "8020:8020"  # RPC service
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    container_name: datanode
    hostname: datanode
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020  #  Correct port
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    depends_on:
      - namenode
    

  ingestion:
    build:
      context: ./ingestion  # Adjust this if your Dockerfile is elsewhere
    container_name: ingestion
    depends_on:
      - namenode
    environment:
      - HDFS_URL=http://namenode:50070  # For WebHDFS client in Python
      - HDFS_USER=hadoop
    volumes:
      - ./data:/data:ro
    command: ["python", "ingest.py"]
    

  spark:
    build:
      context: ./processing
    container_name: spark_processor
    depends_on:
      - namenode
    environment:
      - SPARK_MODE=client
    volumes:
      - ./processing:/app

  data_layer:
    build:
      context: ./data_layer
    depends_on:
      - namenode
      - mongodb
  mongodb:
    image: mongo:6
    container_name: mongodb
    volumes:
      - mongo_data:/data/db
    ports:
      - "27017:27017"   


volumes:
  hadoop_namenode:
  hadoop_datanode:
  mongo_data:

networks:
  hadoop:
   driver: bridge